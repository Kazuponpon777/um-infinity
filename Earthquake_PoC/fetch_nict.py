"""
UM_Infinity V25 Ionosphere - NICT Data Fetcher (Japan)
======================================================
æ—¥æœ¬ã®NICTï¼ˆæƒ…å ±é€šä¿¡ç ”ç©¶æ©Ÿæ§‹ï¼‰ã‹ã‚‰å®‡å®™å¤©æ°—æƒ…å ±ã‚’å–å¾—ã™ã‚‹ã€‚
æˆ‘ã€…ã®ç©ºï¼ˆè±Šæ©‹ä¸Šç©ºï¼‰ã‚’å®ˆã‚‹ãŸã‚ã®é‡è¦ãªãƒ­ãƒ¼ã‚«ãƒ©ã‚¤ã‚ºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚

Source: https://swc.nict.go.jp/
"""

import requests
import re
from typing import Dict, Any, Optional

# NICT Targets
NICT_BASE_URL = "https://swc.nict.go.jp/"
NICT_IONOSPHERE_URL = "https://swc.nict.go.jp/trend/ionosphere.html"

# Timeout settings
REQUEST_TIMEOUT = 10

def _fetch_html(url: str) -> Optional[str]:
    """æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰HTMLã‚’å–å¾—ã™ã‚‹"""
    try:
        response = requests.get(url, timeout=REQUEST_TIMEOUT)
        response.encoding = response.apparent_encoding  # æ—¥æœ¬èªæ–‡å­—åŒ–ã‘é˜²æ­¢
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"[V25 NICT] âš ï¸ Connection failed to {url}: {e}")
        return None

def _analyze_text_risk(text: str, context_label: str = "") -> int:
    """
    ãƒ†ã‚­ã‚¹ãƒˆå†…ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«ã‚’åˆ¤å®šã™ã‚‹ (0-3)
    
    Level 3: è‡¨æ™‚è­¦å ±, æ¿€ã—ã„, è­¦å ± (Alert)
    Level 2: æ´»ç™º (Active)
    Level 1: ã‚„ã‚„æ´»ç™º (Unstable?)
    Level 0: é™ç©, ãƒ‡ãƒ¼ã‚¿ãªã— (Quiet)
    """
def _analyze_text_risk(text: str, context_label: str = "") -> int:
    """
    ãƒ†ã‚­ã‚¹ãƒˆå†…ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«ã‚’åˆ¤å®šã™ã‚‹ (0-3)
    
    Level 3: è‡¨æ™‚è­¦å ±, æ¿€ã—ã„, è­¦å ± (Alert)
    Level 2: æ´»ç™º (Active)
    Level 1: ã‚„ã‚„æ´»ç™º (Unstable?)
    Level 0: é™ç©, ãƒ‡ãƒ¼ã‚¿ãªã— (Quiet)
    """
    # 1. Cleaning: Remove Footer, Header, Nav, HEAD to avoid false positives
    # Simple regex to remove common structural blocks (non-greedy)
    cleaned_text = re.sub(r'<head>.*?</head>', '', text, flags=re.DOTALL)
    cleaned_text = re.sub(r'<footer.*?</footer>', '', cleaned_text, flags=re.DOTALL)
    cleaned_text = re.sub(r'<nav.*?</nav>', '', cleaned_text, flags=re.DOTALL)
    cleaned_text = re.sub(r'class="header".*?</div>', '', cleaned_text, flags=re.DOTALL)
    
    # Specific known false positives
    cleaned_text = cleaned_text.replace("è‡¨æ™‚æƒ…å ±ã®ç™ºä»¤åŸºæº–", "") \
                               .replace("è­¦å ±ã®åŸºæº–", "") \
                               .replace("è­¦å ±ãƒ»æ³¨æ„å ±", "") \
                               .replace("<span>è‡¨æ™‚æƒ…å ±</span>", "") # Menu item

    # 2. Keyword Counting
    # If a real alert exists, these words should appear in the main content area.
    
    # High Risk
    count_alert = cleaned_text.count("è‡¨æ™‚æƒ…å ±") + cleaned_text.count("è­¦å ±") + cleaned_text.count("æ¿€ã—ã„")
    
    # Medium Risk
    count_active = cleaned_text.count("æ´»ç™º")
    
    # Low Risk (Safe)
    count_quiet = cleaned_text.count("é™ç©")
    
    # Logic:
    # If "Alert" appears significantly, treat as Level 3.
    # Note: "è­¦å ±" might appear in "è­¦å ±ã¯ã‚ã‚Šã¾ã›ã‚“" (No warnings). 
    # So we should check for "è­¦å ±" but assume safe if "é™ç©" dominates OR logic for "No Warning".
    
    # Improvement: Check for "No Warning" phrases
    if "è­¦å ±ã¯ã‚ã‚Šã¾ã›ã‚“" in cleaned_text or "è­¦å ±ç­‰ã¯ã‚ã‚Šã¾ã›ã‚“" in cleaned_text:
        count_alert = 0
    
    if count_alert > 0:
        # Check for negation if possible, but Japanese is hard to parse simply.
        # Assuming if "è­¦å ±" appears in body (cleaned), it is likely an alert.
        # However, let's be conservative: if Quiet > Alert, maybe it's okay? 
        # But usually "Quiet" is for specific params, "Alert" is for overall.
        return 3
        
    if count_active > 0:
        return 2
        
    if "ã‚„ã‚„æ´»ç™º" in cleaned_text:
        return 1
    
    return 0

def get_nict_data() -> Dict[str, Any]:
    """
    NICTã‹ã‚‰é›»é›¢åœãŠã‚ˆã³ãƒ‡ãƒªãƒ³ã‚¸ãƒ£ãƒ¼ç¾è±¡ã®çŠ¶æ³ã‚’å–å¾—ã™ã‚‹ã€‚
    
    Returns:
        dict: {
            "ionosphere_level": int (0-3),
            "dellinger_level": int (0-3),
            "risk_score": float (0.0 - 10.0),
            "source": "NICT",
            "location": "Japan/Aichi/Toyohashi"
        }
    """
    print(f"[V25 NICT] ğŸ‡¯ğŸ‡µ Fetching Space Weather Data from NICT...")
    
    # 1. Fetch Top Page or Trend Page
    # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã®ã€Œç¾æ³ã€æ¬„ãŒæœ€ã‚‚æƒ…å ±ãŒã¾ã¨ã¾ã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŒã€
    # è©³ç´°ãƒšãƒ¼ã‚¸ã®æ–¹ãŒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ‹¾ã„ã‚„ã™ã„å ´åˆã‚‚ã‚ã‚‹ã€‚
    # ä»Šå›ã¯ç¢ºå®Ÿæ€§ã‚’é«˜ã‚ã‚‹ãŸã‚ã€ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‹ã‚‰å…¨ä½“æ¦‚æ³ã‚’å–å¾—ã™ã‚‹æˆ¦ç•¥ã‚’ã¨ã‚‹ã€‚
    html = _fetch_html(NICT_BASE_URL)
    
    if not html:
        return {
            "ionosphere_level": 0,
            "dellinger_level": 0,
            "risk_score": None, # Error state
            "error": "Connection Failed",
            "source": "NICT (Offline)"
        }
    
    # 2. Parse Specific Sections
    # HTMLæ§‹é€ è§£æã¯å£Šã‚Œã‚„ã™ã„ãŸã‚ã€ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‘¨è¾ºã‚’æ­£è¦è¡¨ç¾ã§åˆ‡ã‚Šå‡ºã™
    
    # --- é›»é›¢åœ (Ionosphere) ---
    # ç›´è¿‘ã®é›»é›¢åœæ¦‚æ³ã‚’æ¢ã™
    # ä¾‹: "é›»é›¢åœåµ: é™ç©" ã¿ãŸã„ãªè¨˜è¿°ã‚’æ¢ã—ãŸã„ãŒã€HTMLæ§‹é€ ã«ã‚ˆã‚‹ã€‚
    # ã“ã“ã§ã¯HTMLå…¨ä½“ã‹ã‚‰ãƒªã‚¹ã‚¯ãƒ¯ãƒ¼ãƒ‰ã‚’æ¢ã™ãŒã€èª¤æ¤œçŸ¥ã‚’é˜²ããŸã‚
    # "é›»é›¢åœ" ã¨ã„ã†å˜èªã®å¾Œã®ä¸€å®šæ–‡å­—æ•°ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã™ã‚‹
    ionosphere_level = 0
    ionosphere_matches = re.search(r'é›»é›¢åœ.*?<\/a>', html, re.DOTALL) # ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ç­‰ã®ãƒªãƒ³ã‚¯æ–‡å­—ã‹ã‚‚ã—ã‚Œãªã„ãŒ...
    
    # ã‚ˆã‚Šåºƒç¯„å›²ã«æ¤œç´¢: 'é›»é›¢åœåµ' ã¨ã„ã†å˜èªãŒå«ã¾ã‚Œã‚‹ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¢ã™
    # ã‚·ãƒ³ãƒ—ãƒ«ã«ã€ãƒšãƒ¼ã‚¸å…¨ä½“ã®é »å‡ºãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰æ¨å®šã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰
    # â€»å³å¯†ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚ˆã‚Šã€"è­¦å ±"ãŒå‡ºã¦ã„ã‚‹ã‹ã©ã†ã‹ãŒæœ€é‡è¦
    
    # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã«ã€Œè­¦å ±ã€ã‚„ã€Œè‡¨æ™‚æƒ…å ±ã€ãŒå‡ºã¦ã„ã‚‹å ´åˆã¯å…¨ä½“ãƒªã‚¹ã‚¯ã¨ã™ã‚‹
    top_alert_level = _analyze_text_risk(html[:5000]) # ãƒ˜ãƒƒãƒ€ãƒ¼ä»˜è¿‘ã®é‡è¦æƒ…å ±
    
    # å€‹åˆ¥ãƒšãƒ¼ã‚¸ã‚‚ç¢ºèªï¼ˆå¿µã®ãŸã‚ï¼‰
    trend_html = _fetch_html(NICT_IONOSPHERE_URL)
    if trend_html:
        ionosphere_level = _analyze_text_risk(trend_html)
    else:
        ionosphere_level = top_alert_level

    # --- ãƒ‡ãƒªãƒ³ã‚¸ãƒ£ãƒ¼ç¾è±¡ ---
    # ç¾æ™‚ç‚¹ã§ã¯å€‹åˆ¥ã®ãƒ‡ãƒªãƒ³ã‚¸ãƒ£ãƒ¼æƒ…å ±ã‚’å–å¾—ã™ã‚‹URLã‚’å©ãã‹ã€ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã®"ãƒ‡ãƒªãƒ³ã‚¸ãƒ£ãƒ¼"å‘¨è¾ºã‚’è¦‹ã‚‹
    # ç°¡æ˜“çš„ã«ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸åˆ¤å®šçµæœã‚’åˆ©ç”¨ï¼ˆã‚‚ã—ãƒˆãƒƒãƒ—ã«è­¦å ±ãŒã‚ã‚Œã°ãƒ‡ãƒªãƒ³ã‚¸ãƒ£ãƒ¼ã®å¯èƒ½æ€§ã‚‚å«ã‚€ãŸã‚ï¼‰
    dellinger_level = top_alert_level # ä»®ã®å®Ÿè£…
    
    # 3. Calculate Final Risk Score
    # Max Level ã‚’æ¡ç”¨
    max_level = max(ionosphere_level, dellinger_level, top_alert_level)
    
    # Mapping
    # 0 -> 0.0
    # 1 -> 2.5
    # 2 -> 5.0
    # 3 -> 10.0
    risk_map = {0: 0.0, 1: 2.5, 2: 5.0, 3: 10.0}
    risk_score = risk_map.get(max_level, 0.0)
    
    result = {
        "ionosphere_level": ionosphere_level,
        "dellinger_level": dellinger_level,
        "risk_score": risk_score,
        "source": "NICT (National Institute of Information and Communications Technology)",
        "location": "Japan/Aichi/Toyohashi"
    }
    
    print(f"[V25 NICT] Result: Level={max_level} (Risk {risk_score})")
    return result

if __name__ == "__main__":
    # Test run
    data = get_nict_data()
    print("--------------------------------------------------")
    print(f"Location: {data['location']}")
    print(f"Source:   {data['source']}")
    print(f"Risk:     {data['risk_score']}")
    print(f"Details:  Ionosphere={data['ionosphere_level']}, Dellinger={data['dellinger_level']}")
    print("--------------------------------------------------")
